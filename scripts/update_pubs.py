from scholarly import scholarly, ProxyGenerator
import json
import os
import time

# ğŸ‘‰ ë„ˆê°€ ì¤€ Google Scholar ID
SCHOLAR_ID = "MAB85ZYAAAAJ"

print("START fetching publications...")

# --- Google Scholar ì°¨ë‹¨ ë°©ì§€ ì„¤ì • (í•µì‹¬) ---
pg = ProxyGenerator()
pg.FreeProxies()   # ë¬´ë£Œ í”„ë¡ì‹œ ìë™ ì‚¬ìš©
scholarly.use_proxy(pg)

os.makedirs("data", exist_ok=True)

try:
    # 1) ì €ì ì°¾ê¸°
    author = scholarly.search_author_id(SCHOLAR_ID)
    print("Author found")

    # 2) ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    author = scholarly.fill(author, sections=["publications"])
    publications = author.get("publications", [])
    print("Total publications found:", len(publications))

    pubs = []

    # 3) ë…¼ë¬¸ ìƒì„¸ì •ë³´ í•˜ë‚˜ì”© ê°€ì ¸ì˜¤ê¸°
    for i, pub in enumerate(publications):
        print(f"Fetching {i+1}/{len(publications)}")
        try:
            filled = scholarly.fill(pub)
            bib = filled.get("bib", {})

            journal = bib.get("journal", "") or bib.get("venue", "")
            volume = bib.get("volume", "")
            pages = bib.get("pages", "")
            year = bib.get("pub_year", "")

            pubs.append({
                "title": bib.get("title", ""),
                "authors": bib.get("author", ""),
                "journal": journal,
                "year": year,
                "volume": volume,
                "pages": pages
            })

            time.sleep(2)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ (ì¤‘ìš”)

        except Exception as e:
            print("Skip one publication:", e)

    # 4) ìµœì‹ ìˆœ ì •ë ¬ í›„ ì €ì¥
    pubs_sorted = sorted(pubs, key=lambda x: str(x.get("year", "")), reverse=True)

    with open("data/publications.json", "w", encoding="utf-8") as f:
        json.dump(pubs_sorted, f, ensure_ascii=False, indent=2)

    print("DONE â€” publications.json created!")

except Exception as e:
    print("ERROR:", e)
